{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOTv75TxtRwT6of7ifsbEof",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/conglapgit45/RNN_Sentiment_Analysis_for_Financial_News/blob/main/RNN_Sentiment_Analysis_for_Financial_News.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Sentiment Analysis for Financial News"
      ],
      "metadata": {
        "id": "-pz-HCxs9XVk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install unidecode"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SYc-_J_t-VF2",
        "outputId": "700f6319-7731-4c3c-cd80-2cb98bc3babb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting unidecode\n",
            "  Downloading Unidecode-1.3.8-py3-none-any.whl.metadata (13 kB)\n",
            "Downloading Unidecode-1.3.8-py3-none-any.whl (235 kB)\n",
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/235.5 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m235.5/235.5 kB\u001b[0m \u001b[31m9.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: unidecode\n",
            "Successfully installed unidecode-1.3.8\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "seed = 1\n",
        "torch.manual_seed(seed)\n",
        "\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import re\n",
        "import nltk\n",
        "import unidecode\n",
        "\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pGVQe7M99ZC5",
        "outputId": "bd5c08fa-7c5b-4335-b9dc-bf53feafd51d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!gdown --id 1uYXI4O3oWBA6QC8ZJ-r6yaTTfkdAnl_Q\n",
        "\n",
        "!unzip /content/dataset.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T-tCUtUE9_nE",
        "outputId": "04b413b1-2537-496b-ae24-500385768b52"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  /content/dataset.zip\n",
            "   creating: dataset/\n",
            "  inflating: dataset/all-data.csv    \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Read data\n",
        "\n",
        "dataset_path = 'dataset/all-data.csv'\n",
        "headers = ['sentiment', 'content']\n",
        "df = pd.read_csv(\n",
        "    dataset_path,\n",
        "    names = headers,\n",
        "    encoding = 'ISO-8859-1'\n",
        ")\n",
        "\n",
        "classes = {class_name: idx for idx, class_name in enumerate(df['sentiment'].unique().tolist())}\n",
        "df['sentiment'] = df['sentiment'].apply(lambda x: classes[x])"
      ],
      "metadata": {
        "id": "tD-jBowo-rwz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# preprocess data\n",
        "\n",
        "english_stop_words = stopwords.words('english')\n",
        "stemmer = PorterStemmer()\n",
        "\n",
        "def text_normalize(text):\n",
        "    text = text.lower()\n",
        "    text = unidecode.unidecode(text)\n",
        "    text = text.strip()\n",
        "    text = re.sub(r'[^\\w\\s]', '', text)\n",
        "    text = ' '.join([word for word in text.split(' ') if word not in english_stop_words])\n",
        "    text = ' '.join([stemmer.stem(word) for word in text.split(' ')])\n",
        "    return text\n",
        "\n",
        "df['content'] = df['content'].apply(lambda x: text_normalize(x))"
      ],
      "metadata": {
        "id": "xlo-sGPsA_NX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Build vocabulary\n",
        "\n",
        "vocab = []\n",
        "for sentence in df['content'].tolist():\n",
        "    tokens = sentence.split()\n",
        "    for token in tokens:\n",
        "        if token not in vocab:\n",
        "            vocab.append(token)\n",
        "\n",
        "vocab.append('UNK')\n",
        "vocab.append('PAD')\n",
        "word_to_idx = {word: idx for idx, word in enumerate(vocab)}\n",
        "vocab_size = len(vocab)\n",
        "\n",
        "def transform(text, word_to_idx, max_seq_len):\n",
        "    tokens = []\n",
        "    for w in text.split():\n",
        "        try:\n",
        "            w_ids = word_to_idx[w]\n",
        "        except:\n",
        "            w_ids = word_to_idx['UNK']\n",
        "            tokens.append(w_ids)\n",
        "\n",
        "    if len(tokens) < max_seq_len:\n",
        "        tokens += [word_to_idx['PAD']] * (max_seq_len - len(tokens))\n",
        "    elif len(tokens) > max_seq_len:\n",
        "        tokens = tokens[:max_seq_len]\n",
        "\n",
        "    return tokens"
      ],
      "metadata": {
        "id": "igUpdJAMB-2d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Split data to train, val, test\n",
        "\n",
        "val_size = 0.2\n",
        "test_size = 0.125\n",
        "is_shuffle = True\n",
        "texts = df['content'].tolist()\n",
        "labels = df['sentiment'].tolist()\n",
        "\n",
        "X_train, X_val, y_train, y_val = train_test_split(\n",
        "    texts,\n",
        "    labels,\n",
        "    test_size = val_size,\n",
        "    random_state = seed,\n",
        "    shuffle = is_shuffle\n",
        ")\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X_train,\n",
        "    y_train,\n",
        "    test_size = val_size,\n",
        "    random_state =seed,\n",
        "    shuffle = is_shuffle\n",
        ")"
      ],
      "metadata": {
        "id": "6uQFyMncFXyk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Build pytorch dataset\n",
        "\n",
        "class FinancialNews(Dataset):\n",
        "    def __init__(self, X, y, word_to_idx, max_seq_len, transform = None):\n",
        "        self.texts = X\n",
        "        self.labels = y\n",
        "        self.word_to_idx = word_to_idx\n",
        "\n",
        "        self.max_seq_len = max_seq_len\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        text = self.texts[idx]\n",
        "        label = self.labels[idx]\n",
        "\n",
        "        if self.transform:\n",
        "            text = self.transform(\n",
        "                text,\n",
        "                self.word_to_idx,\n",
        "                self.max_seq_len\n",
        "            )\n",
        "        text = torch.tensor(text)\n",
        "\n",
        "        return text , label"
      ],
      "metadata": {
        "id": "Kl_LuGNH-r4m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Declare dataloader\n",
        "\n",
        "max_seq_len = 32\n",
        "\n",
        "train_dataset = FinancialNews(\n",
        "    X_train,\n",
        "    y_train,\n",
        "    word_to_idx = word_to_idx,\n",
        "    max_seq_len = max_seq_len,\n",
        "    transform = transform\n",
        ")\n",
        "val_dataset = FinancialNews(\n",
        "    X_val,\n",
        "    y_val,\n",
        "    word_to_idx = word_to_idx,\n",
        "    max_seq_len = max_seq_len,\n",
        "    transform = transform\n",
        ")\n",
        "test_dataset = FinancialNews(\n",
        "    X_test,\n",
        "    y_test,\n",
        "    word_to_idx = word_to_idx,\n",
        "    max_seq_len = max_seq_len,\n",
        "    transform = transform\n",
        ")\n",
        "\n",
        "train_batch_size = 128\n",
        "test_batch_size = 8\n",
        "\n",
        "train_loader = DataLoader(\n",
        "    train_dataset,\n",
        "    batch_size = train_batch_size,\n",
        "    shuffle = True\n",
        ")\n",
        "\n",
        "val_loader = DataLoader(\n",
        "    val_dataset,\n",
        "    batch_size = test_batch_size,\n",
        "    shuffle = False\n",
        ")\n",
        "\n",
        "test_loader = DataLoader(\n",
        "    test_dataset,\n",
        "    batch_size = test_batch_size,\n",
        "    shuffle = False\n",
        ")"
      ],
      "metadata": {
        "id": "NQ4i4unCIRc-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Buil model\n",
        "\n",
        "class SentimentClassifier(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim, hidden_size, n_layers, n_classes, dropout_prob):\n",
        "        super(SentimentClassifier, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "        self.rnn = nn.RNN(embedding_dim, hidden_size, n_layers, batch_first=True)\n",
        "        self.norm = nn.LayerNorm(hidden_size)\n",
        "        self.dropout = nn.Dropout(dropout_prob)\n",
        "        self.fc1 = nn.Linear(hidden_size, 16)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.fc2 = nn.Linear(16, n_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.embedding(x)\n",
        "        x, hn = self.rnn(x)\n",
        "        x = x[:, -1, :]\n",
        "\n",
        "        x = self.norm(x)\n",
        "        x = self.dropout(x)\n",
        "        x = self.fc1(x)\n",
        "        x = self.relu (x)\n",
        "        x = self.fc2(x)\n",
        "\n",
        "        return x"
      ],
      "metadata": {
        "id": "RDpSWcuJJGQA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Declare sentiment classifier\n",
        "\n",
        "n_classes = len(list(classes.keys()))\n",
        "embedding_dim = 64\n",
        "hidden_size = 64\n",
        "n_layers = 2\n",
        "dropout_prob = 0.2\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "print('Device: ', device)\n",
        "\n",
        "model = SentimentClassifier(\n",
        "    vocab_size = vocab_size,\n",
        "    embedding_dim = embedding_dim,\n",
        "    hidden_size = hidden_size,\n",
        "    n_layers = n_layers,\n",
        "    n_classes = n_classes,\n",
        "    dropout_prob = dropout_prob\n",
        ").to(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z9pvaUU1JGW6",
        "outputId": "9eaab854-af6a-48a8-a668-3da75a50c7e6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device:  cpu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Setup loss function and optimizer\n",
        "\n",
        "lr = 1e-4\n",
        "epochs = 50\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=lr)"
      ],
      "metadata": {
        "id": "JKZgGAw7KBnV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train model\n",
        "\n",
        "def fit(model, train_loader, val_loader, criterion, optimizer, device, epochs):\n",
        "    train_losses = []\n",
        "    val_losses = []\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        batch_train_losses = []\n",
        "\n",
        "        model.train()\n",
        "        for idx, (inputs, labels) in enumerate(train_loader):\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            batch_train_losses.append(loss.item())\n",
        "\n",
        "        train_loss = sum(batch_train_losses) / len(batch_train_losses)\n",
        "        train_losses.append(train_loss)\n",
        "\n",
        "        val_loss, val_acc = evaluate(\n",
        "            model,\n",
        "            val_loader,\n",
        "            criterion,\n",
        "            device\n",
        "        )\n",
        "        val_losses.append(val_loss)\n",
        "\n",
        "        print(f'EPOCH {epoch + 1}: \\t Train loss: {train_loss:.4f} \\t Val loss: {val_loss:.4f}')\n",
        "\n",
        "    return train_losses, val_losses\n",
        "\n",
        "\n",
        "def evaluate(model, dataloader, criterion, device):\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    losses = []\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in dataloader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            losses.append(loss.item())\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted==labels).sum().item()\n",
        "\n",
        "    loss = sum(losses) / len(losses)\n",
        "    acc = correct / total\n",
        "\n",
        "    return loss, acc\n",
        "\n",
        "\n",
        "train_losses, val_losses = fit(\n",
        "    model,\n",
        "    train_loader,\n",
        "    val_loader,\n",
        "    criterion,\n",
        "    optimizer,\n",
        "    device,\n",
        "    epochs\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mzgzahz8KdO5",
        "outputId": "31186d8d-b447-4b5a-d7be-a9aed38759b3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EPOCH 1: \t Train loss: 0.9316 \t Val loss: 0.9290\n",
            "EPOCH 2: \t Train loss: 0.9316 \t Val loss: 0.9289\n",
            "EPOCH 3: \t Train loss: 0.9329 \t Val loss: 0.9291\n",
            "EPOCH 4: \t Train loss: 0.9329 \t Val loss: 0.9291\n",
            "EPOCH 5: \t Train loss: 0.9319 \t Val loss: 0.9290\n",
            "EPOCH 6: \t Train loss: 0.9285 \t Val loss: 0.9294\n",
            "EPOCH 7: \t Train loss: 0.9309 \t Val loss: 0.9291\n",
            "EPOCH 8: \t Train loss: 0.9330 \t Val loss: 0.9290\n",
            "EPOCH 9: \t Train loss: 0.9270 \t Val loss: 0.9291\n",
            "EPOCH 10: \t Train loss: 0.9304 \t Val loss: 0.9290\n",
            "EPOCH 11: \t Train loss: 0.9323 \t Val loss: 0.9289\n",
            "EPOCH 12: \t Train loss: 0.9245 \t Val loss: 0.9295\n",
            "EPOCH 13: \t Train loss: 0.9324 \t Val loss: 0.9291\n",
            "EPOCH 14: \t Train loss: 0.9284 \t Val loss: 0.9291\n",
            "EPOCH 15: \t Train loss: 0.9335 \t Val loss: 0.9289\n",
            "EPOCH 16: \t Train loss: 0.9294 \t Val loss: 0.9295\n",
            "EPOCH 17: \t Train loss: 0.9291 \t Val loss: 0.9291\n",
            "EPOCH 18: \t Train loss: 0.9288 \t Val loss: 0.9290\n",
            "EPOCH 19: \t Train loss: 0.9327 \t Val loss: 0.9289\n",
            "EPOCH 20: \t Train loss: 0.9312 \t Val loss: 0.9291\n",
            "EPOCH 21: \t Train loss: 0.9364 \t Val loss: 0.9292\n",
            "EPOCH 22: \t Train loss: 0.9281 \t Val loss: 0.9292\n",
            "EPOCH 23: \t Train loss: 0.9284 \t Val loss: 0.9293\n",
            "EPOCH 24: \t Train loss: 0.9289 \t Val loss: 0.9295\n",
            "EPOCH 25: \t Train loss: 0.9274 \t Val loss: 0.9292\n",
            "EPOCH 26: \t Train loss: 0.9287 \t Val loss: 0.9293\n",
            "EPOCH 27: \t Train loss: 0.9307 \t Val loss: 0.9291\n",
            "EPOCH 28: \t Train loss: 0.9249 \t Val loss: 0.9290\n",
            "EPOCH 29: \t Train loss: 0.9352 \t Val loss: 0.9290\n",
            "EPOCH 30: \t Train loss: 0.9321 \t Val loss: 0.9290\n",
            "EPOCH 31: \t Train loss: 0.9282 \t Val loss: 0.9293\n",
            "EPOCH 32: \t Train loss: 0.9369 \t Val loss: 0.9289\n",
            "EPOCH 33: \t Train loss: 0.9298 \t Val loss: 0.9289\n",
            "EPOCH 34: \t Train loss: 0.9336 \t Val loss: 0.9289\n",
            "EPOCH 35: \t Train loss: 0.9322 \t Val loss: 0.9291\n",
            "EPOCH 36: \t Train loss: 0.9343 \t Val loss: 0.9289\n",
            "EPOCH 37: \t Train loss: 0.9300 \t Val loss: 0.9290\n",
            "EPOCH 38: \t Train loss: 0.9338 \t Val loss: 0.9293\n",
            "EPOCH 39: \t Train loss: 0.9318 \t Val loss: 0.9290\n",
            "EPOCH 40: \t Train loss: 0.9314 \t Val loss: 0.9292\n",
            "EPOCH 41: \t Train loss: 0.9302 \t Val loss: 0.9291\n",
            "EPOCH 42: \t Train loss: 0.9278 \t Val loss: 0.9291\n",
            "EPOCH 43: \t Train loss: 0.9311 \t Val loss: 0.9290\n",
            "EPOCH 44: \t Train loss: 0.9384 \t Val loss: 0.9292\n",
            "EPOCH 45: \t Train loss: 0.9342 \t Val loss: 0.9294\n",
            "EPOCH 46: \t Train loss: 0.9308 \t Val loss: 0.9292\n",
            "EPOCH 47: \t Train loss: 0.9297 \t Val loss: 0.9289\n",
            "EPOCH 48: \t Train loss: 0.9276 \t Val loss: 0.9292\n",
            "EPOCH 49: \t Train loss: 0.9258 \t Val loss: 0.9289\n",
            "EPOCH 50: \t Train loss: 0.9295 \t Val loss: 0.9291\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate model\n",
        "\n",
        "val_loss, val_acc = evaluate(\n",
        "    model,\n",
        "    val_loader,\n",
        "    criterion,\n",
        "    device\n",
        ")\n",
        "\n",
        "test_loss, test_acc = evaluate(\n",
        "    model,\n",
        "    test_loader,\n",
        "    criterion,\n",
        "    device\n",
        ")\n",
        "\n",
        "print('Evaluation on val / test dataset')\n",
        "print('Val accuracy: ', val_acc)\n",
        "print('Test accuracy: ', test_acc)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BCTzc0rhNQBq",
        "outputId": "4b36ef27-7203-4919-ca64-fbd906890de3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluation on val / test dataset\n",
            "Val accuracy:  0.5876288659793815\n",
            "Test accuracy:  0.6121134020618557\n"
          ]
        }
      ]
    }
  ]
}